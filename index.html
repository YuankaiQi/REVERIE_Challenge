<!DOCTYPE HTML>
<html>
<head>
<title>REVERIE Challenge</title>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
<link rel="stylesheet" href="static/css/main.css" />
</head>
<body>

<!-- Header -->
<header id="header">
  <div class="inner">
    <a href="index.html" class="logo">REVERIE</a>
    <nav id="nav">
      <a href="" style="background:#FF0000; padding-top:17px; padding-bottom:17px">Home</a>
      <a href="challenge_2022.html">Challenge</a>
      <a href="leaderboard.html">Leaderboard</a>
      <a href="people_2022.html" >People</a>
    </nav>
  </div>
</header>
<a href="#menu" class="navPanelToggle"><span class="fa fa-bars"></span></a>

<!-- Banner -->

<section>
  <div class="inner">
    <h1 style="color:#0f4c8a">REVERIE Challenge @ CSIG 2022 </h1>
    <h2 style="color:#0f4c8a">New data, New channel, New rules! </h2>
    <h2 style="color:#0f4c8a">Total Prizes: 200,000RMB (~31,500USD) supported by SK Group, South Korean</h2>
    <h3 style="color:red">Update 21/04/2022: (1) Submision method is changed. See the "Submission" section on the <a href="challenge_2022.html">Challenge</a> page. (2) The referring expression grounding model for channel 1 is released. See the "Channel" section on the <a href="challenge_2022.html">Challenge</a> page.</h3>
  </div>
</section>

<br><br>

<!-- One -->
<section id="intro">
  <div class="inner">
    <h3 class='subtitle'>Introduction</h3> 
    <p>The objective of REVERIE Challenge is to benchmark the state-of-the-art for the remote object grounding task defined in the <a href="https://arxiv.org/abs/1904.10151">paper</a>, in the hope that it might drive progress towards more flexible and powerful human interactions with robots. The REVERIE task requires an intelligent agent to correctly localise a remote target object (can not be observed at the starting location) specified by a concise high-level natural language instruction, such as 'bring me the blue cushion from the sofa in the living room'. In distinction to other embodied tasks such as Vision-and-Language Navigation (VLN) and Embodied Question Answering (EQA), REVERIE evaluates the success based on explicit object grounding rather than the point navigation in VLN or the question answering in EQA. This more clearly reflects the necessity of robots' capability of natural language understanding, visual navigation, and object grounding. More importantly, the concise instructions in REVERIE represent more practical tasks that humans would ask a robot to perform. Those high-level instructions fundamentally differ from the fine-grained visuomotor instructions in VLN, and would empower high-level reasoning and real-world applications.
Moreover, compared to the task of Referring Expression (RefExp) that selects the desired object from a single image, REVERIE is far more challenging in the sense that the target object is not visible in the initial view and needs to be discovered by actively navigating in the environment. Hence, in REVERIE, there are at least an order of magnitude more object candidates to choose from.</p>
    <p> &emsp;&emsp;</p>
       </div>
</section>


<br><br>

<section id="new">
  <div class="inner">
    <h3 class='subtitle'>New data, New channel, New rules</h3>
    <b>&middot;New data:</b> <br> Instead of only considering objects within 3 meters to the viewpoint, in this challenge ALL visible objects are considered for object grounding. <br>
    <b>&middot;New channel:</b> <br> This year we set two channels: Channel 1 uses our referring expression grounding model <a href="https://github.com/zhaoc5/Grounding-REVERIE-Challenge">here</a>,
    and Channel 2 uses your own referring expression grounding model.<br>
    <b>&middot;New rules:</b>
    <ol>
      <li>For each channel, one participant can only be in one team. Each team can have at most six members.</li>
      <li>Send your results on test split to reverie.challenge@gmail.com, and we will evaluate the results for you. You can submit only 5 times totally for the challenge. We provide an evaluation script <a href="https://github.com/YanyuanQiao/HOP-REVERIE-Challenge/blob/main/r2r_src/eval.py">here</a> for self-evaluation on val_seen and val_unseen splits. </li>
      <li>Technical report must be sent to reverie.challenge@gmail.com before the challenge deadline. A template will be provided. </li>
      <li>Results must be better than our baseline (see <a href="leaderboard.html">Leaderboard</a>).</li>
      <li>Refer to the <a href="challenge_2022.html" > Challenge</a> page for more details.</li>
    </ol>
          </p>
  </div>
</section>


<br><br>

<section id="dates">
  <div class="inner">
    <h3 class='subtitle'>Important Dates</h3>
    <b>&middot;Challenge starts: April 15, 2022, 0:00 UTC-0</b> <br>
    <b>&middot;Submission deadline: July 31, 2022, 23:59 UTC-0</b>  <br>
    <b>&middot;Results Notice: August 5, 2022</b> <br>
    <b>&middot;Award Ceremony: One day between 19~21 August 2022</b> </p>
  </div>
</section>

<br><br>

<section id="results">
  <div class="inner">
    <h3 class='subtitle'>Results of REVERIE Challenge 2021</h3>
    <p><b>&middot; Winner Team: </b> Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schimid, and Ivan Laptev<br>
      <b>&middot; Winner Tech Report:</b> download from <a href="https://github.com/YuankaiQi/REVERIE_Challenge/blob/master/2021-Winner-Tech-Report.pdf" target="_blank" download="2020-Winner-Tech-Report.pdf">here</a><br>
    </p>
  </div>
  <div class="inner">
    <h3 class='subtitle'>Results of REVERIE Challenge 2020</h3>
    <p><b>&middot; Winner Team: </b> Chen Gao, Jinyu Chen, Erli Meng, Liang Shi, Xiao Lu, and Si Liu <br>
      <b>&middot; Winner Tech Report:</b> download from <a href="https://github.com/YuankaiQi/REVERIE_Challenge/blob/master/2020-Winner-Tech-Report.pdf" target="_blank" download="2020-Winner-Tech-Report.pdf">here</a><br>
      <b>&middot; Winner Talk:</b>
    </p>
      <p align="center"><iframe width="560" height="315" src="https://www.youtube.com/embed/cCqKQbjNJBc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
         </p>
  </div>
</section>
  
<br><br>
<!--
<section>
  <div class="inner">
    <h5>Please cite our paper as below if you use data or code of REVERIE.</h5>
    <pre><code>
@inproceedings{reverie,
  title={REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments},
  author={Yuankai Qi and Qi Wu and Peter Anderson and Xin Wang and William Yang Wang and Chunhua Shen and Anton van den Hengel},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020}
}
  </code></pre>
 </div>
</section>
-->



<!-- Scripts -->
<script src="/static/js/jquery.min.js"></script>
<script src="/static/js/skel.min.js"></script>
<script src="/static/js/util.js"></script>
<script src="/static/js/main.js"></script>

</body>
</html>
