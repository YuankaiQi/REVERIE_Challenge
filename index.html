<!DOCTYPE HTML>
<html>
<head>
<title>REVERIE Challenge</title>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
<link rel="stylesheet" href="static/css/main.css" />
</head>
<body>

<!-- Header -->
<header id="header">
  <div class="inner">
    <a href="index.html" class="logo">REVERIE</a>
    <nav id="nav">
      <a href="" style="background:#FF0000; padding-top:17px; padding-bottom:17px">Home</a>
      <a href="" title="to be udpated">Challenge</a>
      <a href="" title="to be udpated">Dataset</a>
      <a href="" title="to be udpated">People</a>
    </nav>
  </div>
</header>
<a href="#menu" class="navPanelToggle"><span class="fa fa-bars"></span></a>

<!-- Banner -->

<section>
  <div class="inner">
    <h1 style="color:#0f4c8a">REVERIE Challenge @ CSIG 2022 </h1>
    <h2 style="color:#0f4c8a">New data, New channel, New rules! </h2>
    <h2 style="color:#0f4c8a">Total Prizes: 200,000RMB (~31,500USD) supported by SK Group, South Korean</h2>
  </div>
</section>

<br><br>

<!-- One -->
<section id="intro">
  <div class="inner">
    <h3 class='subtitle'>Introduction</h3> 
    <p>The objective of REVERIE Challenge is to benchmark the state-of-the-art for the remote object grounding task defined in the <a href="https://arxiv.org/abs/1904.10151">paper</a>, in the hope that it might drive progress towards more flexible and powerful human interactions with robots. The REVERIE task requires an intelligent agent to correctly localise a remote target object (can not be observed at the starting location) specified by a concise high-level natural language instruction, such as 'bring me the blue cushion from the sofa in the living room'. In distinction to other embodied tasks such as Vision-and-Language Navigation (VLN) and Embodied Question Answering (EQA), REVERIE evaluates the success based on explicit object grounding rather than the point navigation in VLN or the question answering in EQA. This more clearly reflects the necessity of robots' capability of natural language understanding, visual navigation, and object grounding. More importantly, the concise instructions in REVERIE represent more practical tasks that humans would ask a robot to perform (see <a href="dataset.html">Dataset</a> page). Those high-level instructions fundamentally differ from the fine-grained visuomotor instructions in VLN, and would empower high-level reasoning and real-world applications.
Moreover, compared to the task of Referring Expression (RefExp) that selects the desired object from a single image, REVERIE is far more challenging in the sense that the target object is not visible in the initial view and needs to be discovered by actively navigating in the environment. Hence, in REVERIE, there are at least an order of magnitude more object candidates to choose from.</p>
    <p> &emsp;&emsp;</p>
       </div>
</section>


<br><br>

<section id="new">
  <div class="inner">
    <h3 class='subtitle'>New data, New channel, New rules</h3>
    <b>&middot;New data:</b> <br>We provide bounding boxes of all visible objects at each viewpoint. <br>
    <b>&middot;New channel:</b> <br> This year we have two channels: one using our referring expression grounding model (to be released),
    and the other one using your own referring expression grounding model.<br>
    <b>&middot;New rules:</b>
    <ol>
      <li>For each channel, one person can only be in one team. Each team can have at most six members.</li>
      <li>Submitted results must be immediately visible on the leaderboard. When you have multiple submissions, you can set only the best results visible.</li>
      <li>Technical report must be sent to reverie.challenge@gmail.com before the submission deadline.</li>
      <li>Results must be better than our baseline (to be released).</li>
      <li>Refer to the <a href="" title="to be updated"> Challenge</a> page (to be updated) for more details.</li>
    </ol>
          </p>
  </div>
</section>


<br><br>

<section id="dates">
  <div class="inner">
    <h3 class='subtitle'>Important Dates</h3>
    <b>&middot;Challenge starts: April 15, 2022</b> <br>
    <b>&middot;Submission deadline: July 31, 2022</b>  <br>
    <b>&middot;Results Notice: August 5, 2022</b> <br>
    <b>&middot;Award Ceremony: One day between 19~21 August 2022</b> </p>
  </div>
</section>

<br><br>

<section id="results">
  <div class="inner">
    <h3 class='subtitle'>Results of REVERIE Challenge 2021</h3>
    <p><b>&middot; Winner Team: </b> Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schimid, and Ivan Laptev<br>
      <b>&middot; Winner Tech Report:</b> download from <a href="https://github.com/YuankaiQi/REVERIE_Challenge/blob/master/2021-Winner-Tech-Report.pdf" target="_blank" download="2020-Winner-Tech-Report.pdf">here</a><br>
    </p>
  </div>
  <div class="inner">
    <h3 class='subtitle'>Results of REVERIE Challenge 2020</h3>
    <p><b>&middot; Winner Team: </b> Chen Gao, Jinyu Chen, Erli Meng, Liang Shi, Xiao Lu, and Si Liu <br>
      <b>&middot; Winner Tech Report:</b> download from <a href="https://github.com/YuankaiQi/REVERIE_Challenge/blob/master/2020-Winner-Tech-Report.pdf" target="_blank" download="2020-Winner-Tech-Report.pdf">here</a><br>
      <b>&middot; Winner Talk:</b>
    </p>
      <p align="center"><iframe width="560" height="315" src="https://www.youtube.com/embed/cCqKQbjNJBc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
         </p>
  </div>
</section>
  
<br><br>
<!--
<section>
  <div class="inner">
    <h5>Please cite our paper as below if you use data or code of REVERIE.</h5>
    <pre><code>
@inproceedings{reverie,
  title={REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments},
  author={Yuankai Qi and Qi Wu and Peter Anderson and Xin Wang and William Yang Wang and Chunhua Shen and Anton van den Hengel},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020}
}
  </code></pre>
 </div>
</section>
-->



<!-- Scripts -->
<script src="/static/js/jquery.min.js"></script>
<script src="/static/js/skel.min.js"></script>
<script src="/static/js/util.js"></script>
<script src="/static/js/main.js"></script>

</body>
</html>
