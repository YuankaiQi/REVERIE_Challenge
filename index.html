<!DOCTYPE HTML>
<html>
<head>
<title>REVERIE Challenge</title>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
<link rel="stylesheet" href="static/css/main.css" />
</head>
<body>

<!-- Header -->
<header id="header">
  <div class="inner">
    <a href="index.html" class="logo">REVERIE</a>
    <nav id="nav">
      <a href="" style="background:#FF0000; padding-top:17px; padding-bottom:17px">Home</a>
      <a href="challenge.html">Challenge</a>
      <a href="dataset.html">Dataset</a>
      <a href="people.html">People</a>
    </nav>
  </div>
</header>
<a href="#menu" class="navPanelToggle"><span class="fa fa-bars"></span></a>

<!-- Banner -->

<section>
  <div class="inner">
    <h1 style="color:#0f4c8a">REVERIE Challenge @ICCV Workshop 2021 (<a href="https://human-interaction4robotic-navigation.github.io/">HIRV</a>)</span></h1>
  </div>
</section>

<br><br><br>

<!-- One -->
<section id="intro">
  <div class="inner">
    <h3 class='subtitle'>Introduction</h3> 
    <p>The objective of REVERIE Challenge is to benchmark the state-of-the-art for the remote object grounding task defined in the <a href="https://arxiv.org/abs/1904.10151">paper</a>, in the hope that it might drive progress towards more flexible and powerful human interactions with robots. The REVERIE task requires an intelligent agent to correctly localise a remote target object (can not be observed at the starting location) specified by a concise high-level natural language instruction, such as 'bring me the blue cushion from the sofa in the living room'. In distinction to other embodied tasks such as Vision-and-Language Navigation (VLN) and Embodied Question Answering (EQA), REVERIE evaluates the success based on explicit object grounding rather than the point navigation in VLN or the question answering in EQA. This more clearly reflects the necessity of robots' capability of natural language understanding, visual navigation, and object grounding. More importantly, the concise instructions in REVERIE represent more practical tasks that humans would ask a robot to perform (see <a href="dataset.html">Dataset</a> page). Those high-level instructions fundamentally differ from the fine-grained visuomotor instructions in VLN, and would empower high-level reasoning and real-world applications.
Moreover, compared to the task of Referring Expression (RefExp) that selects the desired object from a single image, REVERIE is far more challenging in the sense that the target object is not visible in the initial view and needs to be discovered by actively navigating in the environment. Hence, in REVERIE, there are at least an order of magnitude more object candidates to choose from.</p>
    <p> &emsp;&emsp;</p>
    <br>
       </div>
</section>

<br><br>

<section id="dates">
  <div class="inner">
    <h3 class='subtitle'>Important Dates</h3>
    <p>
      <b>&middot; June 22, 2021:</b> Challenge starts<br>
      <b>&middot; September 22, 2021:</b> Results submission deadline<br>
      <b>&middot; October 8, 2021:</b> Paper submission deadline</p>
  </div>
</section>

<br><br>

<section id="results">
  <div class="inner">
    <h3 class='subtitle'>Results of REVERIE Challenge 2020</h3>
    <p><b>&middot; Winner Team: </b> Chen Gao, Jinyu Chen, Erli Meng, Liang Shi, Xiao Lu, and Si Liu <br>
      <b>&middot; Winner Tech Report:</b> download from <a href="https://github.com/YuankaiQi/REVERIE_Challenge/blob/master/2020-Winner-Tech-Report.pdf" target="_blank" download="2020-Winner-Tech-Report.pdf">here</a><br>
      <b>&middot; Winner Talk:</b>
    </p>
      <p align="center"><iframe width="560" height="315" src="https://www.youtube.com/embed/cCqKQbjNJBc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
         </p>
  </div>
</section>
  
<br><br>

<section>
  <div class="inner">
    <h5>Please cite our paper as below if you use data or code of REVERIE.</h5>
    <pre><code>
@inproceedings{reverie,
  title={REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments},
  author={Yuankai Qi and Qi Wu and Peter Anderson and Xin Wang and William Yang Wang and Chunhua Shen and Anton van den Hengel},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020}
}
  </code></pre>
 </div>
</section>




<!-- Scripts -->
<script src="/static/js/jquery.min.js"></script>
<script src="/static/js/skel.min.js"></script>
<script src="/static/js/util.js"></script>
<script src="/static/js/main.js"></script>

</body>
</html>
